import cv2
import argparse
from modules.object_detection import detect_objects
from modules.text_generation import generate_description

def main(image_path):
    # 1. 이미지 로드
    image = cv2.imread(image_path)
    if image is None:
        print("이미지를 불러올 수 없습니다.")
        return

    # 2. 객체 탐지
    objects = detect_objects(image)

    # 3. 설명 생성
    description = generate_description(objects)

    # 4. 결과 출력
    print("Generated Description:")
    print(description)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--image", type=str, required=True, help="분석할 이미지 경로")
    args = parser.parse_args()

    main(args.image


import cv2

def detect_objects(image):
    # OpenCV를 사용하여 객체 탐지
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    detected_objects = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        detected_objects.append(image[y:y+h, x:x+w])

    return detected_objects

from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image

# Huggingface 모델 로드
model_name = "nlpconnect/vit-gpt2-image-captioning"
model = VisionEncoderDecoderModel.from_pretrained(model_name)
feature_extractor = ViTImageProcessor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def generate_description(objects):
    descriptions = []

    for obj in objects:
        # OpenCV 이미지 -> PIL 이미지
        pil_image = Image.fromarray(cv2.cvtColor(obj, cv2.COLOR_BGR2RGB))
        pixel_values = feature_extractor(pil_image, return_tensors="pt").pixel_values

        # 설명 생성
        output_ids = model.generate(pixel_values)
        description = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        descriptions.append(description)

    return " ".join(descriptions)




